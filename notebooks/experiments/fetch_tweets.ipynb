{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Cramer tweets from Nitter.net\n",
    "Following the deprecation of Twitter's search function (without login), tools like snscrape no longer work (so does the original notebook for twitter scraping). \n",
    "\n",
    "To fetch tweets now, we have to scrape Nitter.net, a privacy friendly alternative front-end for Twitter, which is powered by the GraphQL API from Twitter that is still available.\n",
    "\n",
    "Examples of html of jim cramer's tweets on Nitter.net are stored in `data/tweets/sample_html`.\n",
    "\n",
    "## Format of the tweets:\n",
    "Please find exported tweets in the `data/tweets` folder. The fields are:\n",
    "- time\n",
    "- content\n",
    "- comments\n",
    "- retweets\n",
    "- quotes\n",
    "- hearts\n",
    "\n",
    "## Downstream processing:\n",
    "1. To perform NER on the tweets and extract the tickers\n",
    "2. To perform sentiment analysis on the tweets to extract Crater's sentiment on the tickers\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and setting up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "from rich.logging import RichHandler\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.FileHandler('../logs/fetch_tweets.log'),\n",
    "    #  logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions\n",
    "Various functions are defined to scrape tweets from Nitter.net. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tweet_stats(tweet) -> dict:\n",
    "    \"\"\"Parse tweet stats from a single tweet\n",
    "    Deals with empty stats, comma in numbers, etc.\n",
    "    Ignores 'plays' and 'views' stats\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    stat_icons = [\"icon-comment\", \"icon-retweet\", \"icon-quote\", \"icon-heart\"]\n",
    "    keys = [\"comments\", \"retweets\", \"quotes\", \"hearts\"]\n",
    "\n",
    "    for icon, key in zip(stat_icons, keys):\n",
    "        stat = tweet.find(\"span\", class_=icon)\n",
    "        if stat:\n",
    "            stat_value = stat.find_next(\"div\").get_text(strip=True)\n",
    "            if stat_value:\n",
    "                stats[key] = int(stat.text.strip().replace(\",\", \"\") or 0)\n",
    "            else:\n",
    "                stats[key] = np.nan\n",
    "        else:\n",
    "            stats[key] = np.nan\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "def parse_tweet(tweet) -> dict:\n",
    "    \"\"\"Parse tweet data from a single tweet\n",
    "\n",
    "    \"\"\"\n",
    "    time_element = tweet.find(\"span\", class_=\"tweet-date\")\n",
    "    try:\n",
    "        time = datetime.strptime(\n",
    "            time_element.find(\"a\")[\"title\"], r\"%b %d, %Y · %I:%M %p %Z\"\n",
    "        )\n",
    "    except AttributeError:\n",
    "        LOGGER.warning(\"Failed to parse tweet time: {}\".format(tweet))\n",
    "        time = datetime.strptime(time_element.text, r\"%b %d, %Y · %I:%M %p %Z\")\n",
    "\n",
    "    # LOGGER.info(\"Current tweet time: {}\".format(time))\n",
    "\n",
    "    content = tweet.find(\"div\", class_=\"tweet-content\").text\n",
    "\n",
    "    stats = tweet.find(\"div\", class_=\"tweet-stats\")\n",
    "    # LOGGER.info(stats)\n",
    "    stats_parsed = parse_tweet_stats(stats)\n",
    "\n",
    "    return {\n",
    "        \"time\": time,\n",
    "        \"content\": content,\n",
    "        \"comments\": stats_parsed['comments'],\n",
    "        \"retweets\": stats_parsed['retweets'],\n",
    "        \"quotes\": stats_parsed['quotes'],\n",
    "        \"hearts\": stats_parsed['hearts'],\n",
    "    }\n",
    "\n",
    "\n",
    "def get_next_page_cursor(soup) -> str|None:\n",
    "    \"\"\"Get the next page cursor from the soup object\n",
    "\n",
    "    :param soup: bs4 soup object\n",
    "    :type soup: bs4.BeautifulSoup\n",
    "    :return: next page cursor\n",
    "    :rtype: str or None\n",
    "    \"\"\"\n",
    "    timeline_end = soup.find(\"h2\", class_=\"timeline-end\")\n",
    "    if timeline_end:\n",
    "        return None\n",
    "\n",
    "    load_more_buttons = soup.find_all(\"div\", class_=\"show-more\")\n",
    "    if not load_more_buttons:\n",
    "        return None\n",
    "    load_more = load_more_buttons[-1]\n",
    "    next_page = load_more.find(\"a\")[\"href\"].split(\"cursor=\")[-1]\n",
    "    return next_page\n",
    "\n",
    "\n",
    "def fetch_tweets(user, start_date=None, end_date=None, cursor=None):\n",
    "    \"\"\"Fetch tweets from a user from nitter.net\n",
    "    Specify the start_date and end_date to limit the time range.\n",
    "    Handles pagination.\n",
    "    Skips retweets and show-more buttons\n",
    "\n",
    "\n",
    "    :param user: user name without @\n",
    "    :type user: str\n",
    "    :param start_date: _description_, defaults to None\n",
    "    :type start_date: _type_, optional\n",
    "    :param end_date: _description_, defaults to None\n",
    "    :type end_date: _type_, optional\n",
    "    :param cursor: _description_, defaults to None\n",
    "    :type cursor: _type_, optional\n",
    "    :return: _description_\n",
    "    :rtype: _type_\n",
    "    \"\"\"\n",
    "    base_url = f\"https://nitter.net/{user}/search?f=tweets&e-native_video=on&e-pro_video=on&e-news=on&e-replies=on&e-nativeretweets=on\"\n",
    "    tweets_data = []\n",
    "    page_count = 0\n",
    "\n",
    "    LOGGER.info(\"Start fetching tweets from {}\".format(user))\n",
    "    if start_date:\n",
    "        LOGGER.info(\"Start date: {}\".format(start_date))\n",
    "    if end_date:\n",
    "        LOGGER.info(\"End date: {}\".format(end_date))\n",
    "\n",
    "    while True:\n",
    "        url = base_url\n",
    "        if cursor:\n",
    "            url += f\"&cursor={cursor}\"\n",
    "        LOGGER.info(\"current url: \" + url)\n",
    "\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        tweets = soup.find_all(\"div\", class_=\"timeline-item\")\n",
    "        LOGGER.info(\"Fetched tweet count: {}\".format(len(tweets)))\n",
    "        for tweet in tweets:\n",
    "            # skip retweets and show-more\n",
    "            if (\n",
    "                tweet.find(\"div\", class_=\"retweet-header\")\n",
    "                or \"show-more\" in tweet[\"class\"]\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            tweet_data = parse_tweet(tweet)\n",
    "            tweet_date = tweet_data[\"time\"]\n",
    "            # print(tweet_date)\n",
    "\n",
    "            if start_date and tweet_date < start_date:\n",
    "                LOGGER.info(\"start date reached\")\n",
    "                return cursor, tweets_data\n",
    "            if end_date and tweet_date > end_date:\n",
    "                continue\n",
    "\n",
    "            tweets_data.append(tweet_data)\n",
    "        LOGGER.info(\"finished fetching page {} starts from {}\".format(page_count, tweets_data[-1][\"time\"] if tweets_data else None))\n",
    "        LOGGER.info(\"Total tweet count: {}\".format(len(tweets_data)))\n",
    "        page_count += 1\n",
    "\n",
    "        new_cursor = get_next_page_cursor(soup)\n",
    "        if new_cursor and new_cursor != f\"/{user}\" and cursor != new_cursor:\n",
    "            cursor = new_cursor\n",
    "            LOGGER.info(\"next page = {}, cursor = {}\".format(page_count, cursor))\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return cursor, tweets_data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_yearly_date_ranges(start_date, end_date):\n",
    "    date_ranges = []\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        next_year_start = current_date.replace(year=current_date.year + 1, month=1, day=1)\n",
    "        if next_year_start > end_date:\n",
    "            date_ranges.append((current_date, end_date))\n",
    "        else:\n",
    "            date_ranges.append((current_date, next_year_start - timedelta(days=1)))\n",
    "        current_date = next_year_start\n",
    "    return date_ranges"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main web scraping loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = \"jimcramer\"\n",
    "start_date = datetime(2010, 3, 20)\n",
    "end_date = datetime(2022, 3, 20)\n",
    "\n",
    "# Set the cursor to skip the previously fetched pages\n",
    "last_cursor = \"DAADDAABCgABFuU7axTaYAAKAAIUgjf_sxcwBwAIAAIAAAACCAADAAAAAAgABAAAALAKAAUW5e3Ch8AnEAoABhbl7cKHpP3wAAA\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping jimcramer_tweets_2022-01-01-2022-03-20, already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 2/13 [05:55<32:34, 177.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cursor:  DAADDAABCgABFuU7axTaYAAKAAISrGa_-pbwAQAIAAIAAAACCAADAAAAAAgABAAAAaAKAAUW5e3Ch8AnEAoABhbl7cKHgF7wAAA\n",
      "Scraped 4766 tweets from jimcramer between 2021-01-01 00:00:00 and 2021-12-31 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 3/13 [14:41<53:49, 323.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cursor:  DAADDAABCgABFuU7axTaYAAKAAIQ1IWaVdewAAAIAAIAAAACCAADAAAAAAgABAAAAv0KAAUW5e3Ch8AnEAoABhbl7cKHSx4gAAA\n",
      "Scraped 6946 tweets from jimcramer between 2020-01-01 00:00:00 and 2020-12-31 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 4/13 [20:54<51:13, 341.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cursor:  DAADDAABCgABFuU7axTaYAAKAAIO_1Hwy1dAAAAIAAIAAAACCAADAAAAAAgABAAAA-wKAAUW5e3Ch8AnEAoABhbl7cKHJqYwAAA\n",
      "Scraped 4763 tweets from jimcramer between 2019-01-01 00:00:00 and 2019-12-31 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 5/13 [26:56<46:30, 348.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cursor:  DAADDAABCgABFuU7axTaYAAKAAINKL1WQpfgAAAIAAIAAAACCAADAAAAAAgABAAABNsKAAUW5e3Ch8AnEAoABhbl7cKHAi5AAAA\n",
      "Scraped 4758 tweets from jimcramer between 2018-01-01 00:00:00 and 2018-12-31 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 6/13 [36:03<48:21, 414.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cursor:  DAADDAABCgABFuU7axTaYAAKAAILVAsiyVZAAAAIAAIAAAACCAADAAAAAAgABAAABjoKAAUW5e3Ch8AnEAoABhbl7cKGzJ9QAAA\n",
      "Scraped 6989 tweets from jimcramer between 2017-01-01 00:00:00 and 2017-12-31 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 7/13 [45:36<46:32, 465.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cursor:  DAADDAABCgABFuU7axTaYAAKAAIJfDCL1FawAAAIAAIAAAACCAADAAAAAAgABAAAB6gKAAUW5e3Ch8AnEAoABhbl7cKGlMZwAAA\n",
      "Scraped 7264 tweets from jimcramer between 2016-01-01 00:00:00 and 2016-12-31 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 8/13 [56:47<44:10, 530.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cursor:  DAADDAABCgABFuU7axTaYAAKAAIHqUNNOQhQAAAIAAIAAAACCAADAAAAAAgABAAACXAKAAUW5e3Ch8AnEAoABhbl7cKGTzHwAAA\n",
      "Scraped 9059 tweets from jimcramer between 2015-01-01 00:00:00 and 2015-12-31 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 9/13 [1:04:30<33:57, 509.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cursor:  DAADDAABCgABFuU7axTaYAAKAAIFzppcJggQAAAIAAIAAAACCAADAAAAAAgABAAACscKAAUW5e3Ch8AnEAoABhbl7cKGGtuAAAA\n",
      "Scraped 6863 tweets from jimcramer between 2014-01-01 00:00:00 and 2014-12-31 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 10/13 [1:12:45<25:15, 505.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cursor:  DAADDAABCgABFuU7axTaYAAKAAID-JUsWAJQAQAIAAIAAAACCAADAAAAAAgABAAADDIKAAUW5e3Ch8AnEAoABhbl7cKF43fQAAA\n",
      "Scraped 7262 tweets from jimcramer between 2013-01-01 00:00:00 and 2013-12-31 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 11/13 [1:18:56<15:27, 463.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cursor:  DAADDAABCgABFuU7axTaYAAKAAICJD-X2EIwAQAIAAIAAAACCAADAAAAAAgABAAADUgKAAUW5e3Ch8AnEAoABhbl7cKFuQxwAAA\n",
      "Scraped 5557 tweets from jimcramer between 2012-01-01 00:00:00 and 2012-12-31 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 12/13 [1:23:48<06:51, 411.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cursor:  DAADDAABCgABFuU7axTaYAAKAAIATeXRw0AgAAAIAAIAAAACCAADAAAAAAgABAAADiUKAAUW5e3Ch8AnEAoABhbl7cKFl1OgAAA\n",
      "Scraped 4405 tweets from jimcramer between 2011-01-01 00:00:00 and 2011-12-31 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [1:27:53<00:00, 405.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cursor:  DAADDAABCgABFuU7axTaYAAKAAIAAAACgld-4gAIAAIAAAACCAADAAAAAAgABAAADt0KAAUW5e3Ch8AnEAoABhbl7cKFe0AgAAA\n",
      "Scraped 3676 tweets from jimcramer between 2010-03-20 00:00:00 and 2010-12-31 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "date_ranges = generate_yearly_date_ranges(start_date, end_date)\n",
    "\n",
    "for start, end in tqdm(date_ranges[::-1]):\n",
    "    filename = f\"{user}_tweets_{start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}\"\n",
    "    if os.path.exists(f\"../data/{filename}.csv\"):\n",
    "        print(f\"Skipping {filename}, already exists\")\n",
    "        continue\n",
    "    \n",
    "    last_cursor, tweets_data = fetch_tweets(user, start, end, cursor=last_cursor)\n",
    "    print(\"cursor: \", last_cursor)\n",
    "    tweets_df = pd.DataFrame(tweets_data)\n",
    "\n",
    "    if len(tweets_df) > 0:\n",
    "        tweets_df.to_csv(f\"../data/tweets/{filename}.csv\", index=False)\n",
    "        print(f\"Scraped {len(tweets_df)} tweets from {user} between {start} and {end}\")\n",
    "    else:\n",
    "        print(f\"No tweets found for {user} between {start} and {end}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
